{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize AWS credentials from the .env file\n",
    "AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "AWS_SESSION_TOKEN = os.getenv('AWS_SESSION_TOKEN')\n",
    "AWS_REGION = os.getenv('AWS_REGION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 20:16:40 WARN Utils: Your hostname, Somnium.local resolves to a loopback address: 127.0.0.1; using 172.28.59.194 instead (on interface en0)\n",
      "24/09/30 20:16:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/borja/Library/Caches/pypoetry/virtualenvs/route-optimizer-AqO2e-Ud-py3.11/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/borja/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/borja/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-3bf47d9f-ca99-4dfd-884b-3fa2e0d19e13;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 201ms :: artifacts dl 8ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-3bf47d9f-ca99-4dfd-884b-3fa2e0d19e13\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/5ms)\n",
      "24/09/30 20:16:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Initialize Spark session with Delta and S3 settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KinesisToDeltaLake\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.3.2,com.amazonaws:aws-java-sdk-bundle:1.11.1026\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.session.token\", AWS_SESSION_TOKEN) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"134217728\")  \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize boto3 client for Kinesis with your credentials\n",
    "kinesis_client = boto3.client(\n",
    "    'kinesis',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    aws_session_token=AWS_SESSION_TOKEN,\n",
    "    region_name=AWS_REGION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 path for Delta table\n",
    "delta_table_path = \"s3a://orders-for-dispatch/raw_data\"\n",
    "\n",
    "# Accumulate orders in a list\n",
    "order_buffer = []\n",
    "buffer_limit = 1000  # Limit of accumulated orders before writing\n",
    "flush_interval = 3*60  # Flush the buffer after 60 seconds if limit isn't reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_kinesis_records(stream_name, shard_id):\n",
    "    global order_buffer\n",
    "    shard_iterator = kinesis_client.get_shard_iterator(\n",
    "        StreamName=stream_name,\n",
    "        ShardId=shard_id,\n",
    "        ShardIteratorType='LATEST'\n",
    "    )['ShardIterator']\n",
    "\n",
    "    last_flush_time = time.time()\n",
    "\n",
    "    while True:\n",
    "        # Fetch records from Kinesis\n",
    "        response = kinesis_client.get_records(ShardIterator=shard_iterator, Limit=100)\n",
    "        records = response['Records']\n",
    "\n",
    "        if records:\n",
    "            # Parse Kinesis records and append to the buffer\n",
    "            orders = [json.loads(record['Data']) for record in records]\n",
    "            order_buffer.extend(orders)\n",
    "\n",
    "        # Check if buffer limit or time threshold is reached\n",
    "        if len(order_buffer) >= buffer_limit or (time.time() - last_flush_time) >= flush_interval:\n",
    "            if order_buffer:\n",
    "                # Convert to DataFrame and write to Delta\n",
    "                df = spark.createDataFrame(order_buffer)\n",
    "                \n",
    "                # Optimize file size by controlling partitioning\n",
    "                # Coalesce to a limited number of partitions (e.g., 10)\n",
    "                df.coalesce(10).write.format(\"delta\").mode(\"append\").save(delta_table_path)\n",
    "                \n",
    "                print(f\"Wrote {len(order_buffer)} records to Delta table.\")\n",
    "\n",
    "                # Clear buffer and update flush time\n",
    "                order_buffer.clear()\n",
    "                last_flush_time = time.time()\n",
    "\n",
    "        # Update shard iterator for the next batch\n",
    "        shard_iterator = response['NextShardIterator']\n",
    "        time.sleep(2)  # Sleep to avoid throttling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 20:19:48 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "24/09/30 20:20:00 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 5 records to Delta table.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 20:23:05 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 7 records to Delta table.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 6 records to Delta table.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/30 20:29:23 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote 10 records to Delta table.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Example usage: fetch records from Kinesis and accumulate before writing\n",
    "fetch_kinesis_records('OrderStreamForDispatching', 'shardId-000000000000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def optimize_delta_table():\n",
    "    delta_table_path = \"s3a://orders-for-dispatch/delta_table\"\n",
    "    delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "    delta_table.optimize().executeCompaction()\n",
    "    print(\"Delta table optimized.\")\n",
    "\n",
    "# Run the optimization every hour\n",
    "while True:\n",
    "    optimize_delta_table()\n",
    "    time.sleep(3600)  # Sleep for 1 hour (3600 seconds)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "route-optimizer-AqO2e-Ud-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
