{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize AWS credentials from the .env file\n",
    "AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "AWS_SESSION_TOKEN = os.getenv('AWS_SESSION_TOKEN')\n",
    "AWS_REGION = os.getenv('AWS_REGION')\n",
    "\n",
    "# Initialize the S3 bucket name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your local JAR files\n",
    "local_jars = \"/Users/borja/Documents/Somniumrema/projects/de/route_optimizer/jars/aws-java-sdk-kinesis-1.12.364.jar\"\n",
    "\n",
    "spark = (SparkSession.builder\n",
    "    .master(\"local[*]\")  # Running Spark locally with all available cores\n",
    "    .appName(\"DeltaLakeAggregation\")  # Application name\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")  # Delta Lake extension\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")  # Delta Catalog\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.3.2,com.amazonaws:aws-java-sdk-bundle:1.11.1026\")  # JARs for Delta Lake, S3, and AWS SDK\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")  # S3A file system for S3 access\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID)  # AWS access key\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)  # AWS secret key\n",
    "    .config(\"spark.hadoop.fs.s3a.session.token\", AWS_SESSION_TOKEN)  # AWS session token (if using temporary credentials)\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\")  # S3 endpoint\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"134217728\")  # Max partition size in bytes (128MB)\n",
    "    .config(\"spark.driver.memory\", \"6g\")  # Memory allocated to the Spark driver\n",
    "    .config(\"spark.executor.memory\", \"6g\")  # Memory allocated to Spark executors\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\")  # Disabling Adaptive Query Execution for control\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"100\")  # Increase the max number of fields for debugging output\n",
    "    .config(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\")  # Enable schema auto-merge for Delta Lake\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")  # Enable Arrow optimizations for PySpark\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-XX:+UseParallelGC\")  # Garbage collection optimization\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-XX:+UseParallelGC\")  # Garbage collection optimization for the driver\n",
    "    .getOrCreate())\n",
    "\n",
    "# Optional: Adjust logging level\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "# Read the stream from the Delta Lake (Bronze layer) without specifying schema\n",
    "df_stream = spark.readStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"ignoreDeletes\", \"true\") \\\n",
    "    .option(\"ignoreChanges\", \"true\") \\\n",
    "    .load(\"s3a://orders-for-dispatch/bronze/\")\n",
    "\n",
    "# Add hour-based column for partitioning and change status from 'RECEIVED' to 'READY_FOR_DELIVERY'\n",
    "df_with_hour_and_status = df_stream \\\n",
    "    .withWatermark(\"order_timestamp\", \"15 minute\") \\\n",
    "    .withColumn(\"order_hour\", F.date_format(F.col(\"order_timestamp\"), \"yy-MM-dd-HH\")) \\\n",
    "    .withColumn(\"status\", F.when(F.col(\"status\") == \"RECEIVED\", \"READY_FOR_DELIVERY\").otherwise(F.col(\"status\")))\n",
    "\n",
    "# Write the streaming data to Delta table, partitioning by hour and keeping the row structure\n",
    "query = df_with_hour_and_status.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .partitionBy(\"order_hour\") \\\n",
    "    .option(\"checkpointLocation\", \"s3a://orders-for-dispatch/checkpoints/silver_stream\") \\\n",
    "    .trigger(processingTime=\"5 minutes\") \\\n",
    "    .start(\"s3a://orders-for-dispatch/silver/\")\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for s in spark.streams.active:\n",
    "#     s.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_stream = spark.readStream \\\n",
    "#     .format(\"delta\") \\\n",
    "#     .option(\"ignoreDeletes\", \"true\") \\\n",
    "#     .option(\"ignoreChanges\", \"true\") \\\n",
    "#     .load(\"s3a://orders-for-dispatch/bronze/\")\n",
    "\n",
    "# df_stream.writeStream \\\n",
    "#     .format(\"console\") \\\n",
    "#     .option(\"checkpointLocation\", \"s3a://orders-for-dispatch/checkpoints/bronze_stream\") \\\n",
    "#     .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.streaming import DataStreamWriter\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.streaming.state import GroupState, GroupStateTimeout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# # Define schema for the data\n",
    "# state_schema = StructType([\n",
    "#     StructField(\"order_id\", StringType(), True),\n",
    "#     StructField(\"customer_id\", StringType(), True),\n",
    "#     StructField(\"total_weight\", DoubleType(), True),\n",
    "#     StructField(\"total_volume\", DoubleType(), True),\n",
    "#     StructField(\"total_price\", DoubleType(), True),\n",
    "#     StructField(\"order_timestamp\", TimestampType(), True),\n",
    "#     StructField(\"lat\", DoubleType(), True),\n",
    "#     StructField(\"lon\", DoubleType(), True)\n",
    "# ])\n",
    "\n",
    "# # Define thresholds and lorry capacity\n",
    "# VOLUME_THRESHOLD = 1000  # Minimum volume for dispatch\n",
    "# WEIGHT_THRESHOLD = 500  # Minimum weight for dispatch\n",
    "# MAX_VOLUME_CAPACITY = 20000  # Maximum lorry volume capacity\n",
    "# MAX_WEIGHT_CAPACITY = 5000  # Maximum lorry weight capacity\n",
    "\n",
    "# # Read the stream from the Delta Lake (Bronze layer)\n",
    "# df_stream = spark.readStream \\\n",
    "#     .format(\"delta\") \\\n",
    "#     .option(\"ignoreDeletes\", \"true\") \\\n",
    "#     .option(\"ignoreChanges\", \"true\") \\\n",
    "#     .load(\"s3a://orders-for-dispatch/bronze/\")\n",
    "\n",
    "\n",
    "\n",
    "# # Watermark and group orders within a time window, summing volume and weight\n",
    "# df_with_window = df_stream \\\n",
    "#     .withWatermark(\"order_timestamp\", \"1 minute\") \\\n",
    "#     .groupBy(\n",
    "#         F.window(\"order_timestamp\", \"10 minutes\")  # 10-minute tumbling window\n",
    "#     ).agg(\n",
    "#         F.sum(\"total_volume\").alias(\"accumulated_volume\"),\n",
    "#         F.sum(\"total_weight\").alias(\"accumulated_weight\"),\n",
    "#         F.collect_list(\"order_id\").alias(\"order_id\"),\n",
    "#         F.collect_list(\"customer_id\").alias(\"customer_id\"),\n",
    "#         F.collect_list(\"total_volume\").alias(\"total_volume\"),\n",
    "#         F.collect_list(\"total_weight\").alias(\"total_weight\"),\n",
    "#         F.collect_list(\"lat\").alias(\"lat\"),\n",
    "#         F.collect_list(\"lon\").alias(\"lon\")\n",
    "#     )\n",
    "\n",
    "# # UDF to split orders based on lorry capacity\n",
    "# def split_orders(volume, weight, order_ids, customer_ids, lats, lons):\n",
    "#     batch = {\"order_id\": [], \"customer_id\": [], \"lat\": [], \"lon\": [], \"total_volume\": 0, \"total_weight\": 0}\n",
    "#     leftover = {\"order_id\": [], \"customer_id\": [], \"lat\": [], \"lon\": [], \"total_volume\": 0, \"total_weight\": 0}\n",
    "    \n",
    "#     for v, w, oid, cid, lat, lon in zip(volume, weight, order_ids, customer_ids, lats, lons):\n",
    "#         if (batch[\"volume\"] + v <= MAX_VOLUME_CAPACITY) and (batch[\"weight\"] + w <= MAX_WEIGHT_CAPACITY):\n",
    "#             batch[\"order_ids\"].append(oid)\n",
    "#             batch[\"customer_ids\"].append(cid)\n",
    "#             batch[\"lat\"].append(lat)\n",
    "#             batch[\"lon\"].append(lon)\n",
    "#             batch[\"total_volume\"] += v\n",
    "#             batch[\"total_weight\"] += w\n",
    "#         else:\n",
    "#             leftover[\"order_id\"].append(oid)\n",
    "#             leftover[\"customer_id\"].append(cid)\n",
    "#             leftover[\"lat\"].append(lat)\n",
    "#             leftover[\"lon\"].append(lon)\n",
    "#             leftover[\"total_volume\"] += v\n",
    "#             leftover[\"total_weight\"] += w\n",
    "    \n",
    "#     return batch, leftover\n",
    "\n",
    "# # Register UDF\n",
    "# split_orders_udf = F.udf(split_orders, \n",
    "#     StructType([\n",
    "#         StructField(\"batch\", \n",
    "#             StructType([\n",
    "#                 StructField(\"order_id\", StringType(), True),\n",
    "#                 StructField(\"customer_id\", StringType(), True),\n",
    "#                 StructField(\"lat\", StringType(), True),\n",
    "#                 StructField(\"lon\", StringType(), True),\n",
    "#                 StructField(\"total_volume\", DoubleType(), True),\n",
    "#                 StructField(\"total_weight\", DoubleType(), True)\n",
    "#             ]), \n",
    "#         True),\n",
    "#         StructField(\"remaining\", \n",
    "#             StructType([\n",
    "#                 StructField(\"order_id\", StringType(), True),\n",
    "#                 StructField(\"customer_id\", StringType(), True),\n",
    "#                 StructField(\"lat\", StringType(), True),\n",
    "#                 StructField(\"lon\", StringType(), True),\n",
    "#                 StructField(\"total_volume\", DoubleType(), True),\n",
    "#                 StructField(\"total_weight\", DoubleType(), True)\n",
    "#             ]), \n",
    "#         True)\n",
    "#     ])\n",
    "# )\n",
    "\n",
    "# # Apply the split function on each windowed batch of orders\n",
    "# df_split = df_with_window.withColumn(\n",
    "#     \"split_result\", split_orders_udf(\n",
    "#         F.col(\"total_volume\"), F.col(\"total_weight\"), F.col(\"order_id\"), \n",
    "#         F.col(\"customer_id\"), F.col(\"lat\"), F.col(\"lon\")\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# # Extract the batches and the leftovers\n",
    "# df_batches = df_split.select(\n",
    "#     F.col(\"split_result.batch.order_id\").alias(\"order_id\"),\n",
    "#     F.col(\"split_result.batch.customer_ids\").alias(\"customer_id\"),\n",
    "#     F.col(\"split_result.batch.lats\").alias(\"lat\"),\n",
    "#     F.col(\"split_result.batch.lons\").alias(\"lon\"),\n",
    "#     F.col(\"split_result.batch.total_volume\").alias(\"total_volume\"),\n",
    "#     F.col(\"split_result.batch.total_weight\").alias(\"total_weight\"),\n",
    "#     F.lit(\"READY_FOR_DISPATCH\").alias(\"status\")\n",
    "# )\n",
    "\n",
    "# df_leftovers = df_split.select(\n",
    "#     F.col(\"split_result.remaining.order_id\").alias(\"order_id\"),\n",
    "#     F.col(\"split_result.remaining.customer_id\").alias(\"customer_id\"),\n",
    "#     F.col(\"split_result.remaining.lat\").alias(\"lat\"),\n",
    "#     F.col(\"split_result.remaining.lon\").alias(\"lon\"),\n",
    "#     F.col(\"split_result.remaining.total_volume\").alias(\"total_volume\"),\n",
    "#     F.col(\"split_result.remaining.total_weight\").alias(\"total_weight\")\n",
    "# ).withColumn(\"status\", F.lit(\"RECEIVED\"))\n",
    "\n",
    "# # Union the batches ready for dispatch and the leftover orders\n",
    "# df_final = df_batches.unionByName(df_leftovers)\n",
    "\n",
    "# # Write the result to the Delta Lake (Silver layer)\n",
    "# query = df_final.writeStream \\\n",
    "#     .format(\"delta\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .option(\"checkpointLocation\", \"s3a://orders-for-dispatch/checkpoints/silver_stream\") \\\n",
    "#     .trigger(processingTime=\"10 seconds\") \\\n",
    "#     .start(\"s3a://orders-for-dispatch/silver/\")\n",
    "\n",
    "# query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql import functions as F\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, DoubleType, TimestampType\n",
    "\n",
    "# # Define schema for the data\n",
    "# state_schema = StructType([\n",
    "#     StructField(\"order_id\", StringType(), True),\n",
    "#     StructField(\"customer_id\", StringType(), True),\n",
    "#     StructField(\"total_weight\", DoubleType(), True),\n",
    "#     StructField(\"total_volume\", DoubleType(), True),\n",
    "#     StructField(\"total_price\", DoubleType(), True),\n",
    "#     StructField(\"order_timestamp\", TimestampType(), True),  # TimestampType for time-based operations\n",
    "#     StructField(\"lat\", DoubleType(), True),\n",
    "#     StructField(\"lon\", DoubleType(), True)\n",
    "# ])\n",
    "\n",
    "# # Define the thresholds and maximum dispatch capacity\n",
    "# VOLUME_THRESHOLD = 1000  # Minimum volume for dispatch\n",
    "# WEIGHT_THRESHOLD = 500  # Minimum weight for dispatch\n",
    "# MAX_VOLUME_CAPACITY = 20000  # Maximum lorry capacity in terms of volume\n",
    "# MAX_WEIGHT_CAPACITY = 5000  # Maximum lorry capacity in terms of weight\n",
    "\n",
    "# # Read the stream from the Delta Lake (Bronze layer)\n",
    "# df_stream = spark.readStream \\\n",
    "#     .format(\"delta\") \\\n",
    "#     .option(\"ignoreDeletes\", \"true\") \\\n",
    "#     .option(\"ignoreChanges\", \"true\") \\\n",
    "#     .load(\"s3a://orders-for-dispatch/bronze/\")\n",
    "\n",
    "# # Apply a watermark and group by 10-minute tumbling windows, summing up total volume and weight\n",
    "# df_with_window = df_stream \\\n",
    "#     .withWatermark(\"order_timestamp\", \"1 minutes\") \\\n",
    "#     .groupBy(\n",
    "#         F.window(\"order_timestamp\", \"1 minutes\")  # Time-based tumbling window\n",
    "#     ).agg(\n",
    "#         F.sum(\"total_volume\").alias(\"accumulated_volume\"),\n",
    "#         F.sum(\"total_weight\").alias(\"accumulated_weight\"),\n",
    "#         F.collect_list(\"total_volume\").alias(\"total_volume\"),\n",
    "#         F.collect_list(\"total_weight\").alias(\"total_weight\"),\n",
    "#         F.collect_list(\"order_id\").alias(\"order_ids\"),\n",
    "#         F.collect_list(\"customer_id\").alias(\"customer_ids\"),\n",
    "#         F.collect_list(\"total_price\").alias(\"total_prices\"),\n",
    "#         F.collect_list(\"lat\").alias(\"lats\"),\n",
    "#         F.collect_list(\"lon\").alias(\"lons\")\n",
    "#     )\n",
    "\n",
    "# # Process orders that meet the dispatch threshold and split into batches based on lorry capacity\n",
    "# df_filtered = df_with_window.filter(\n",
    "#     (F.col(\"accumulated_volume\") > VOLUME_THRESHOLD) &\n",
    "#     (F.col(\"accumulated_weight\") > WEIGHT_THRESHOLD)\n",
    "# )\n",
    "\n",
    "# # Split the batches into manageable chunks based on the lorry capacity\n",
    "# df_ready_for_dispatch = df_filtered.withColumn(\n",
    "#     \"ready_volume\", F.when(F.col(\"accumulated_volume\") > MAX_VOLUME_CAPACITY, MAX_VOLUME_CAPACITY).otherwise(F.col(\"accumulated_volume\"))\n",
    "# ).withColumn(\n",
    "#     \"ready_weight\", F.when(F.col(\"accumulated_weight\") > MAX_WEIGHT_CAPACITY, MAX_WEIGHT_CAPACITY).otherwise(F.col(\"accumulated_weight\"))\n",
    "# ).withColumn(\n",
    "#     \"status\", F.lit(\"READY_FOR_DISPATCH\")\n",
    "# )\n",
    "\n",
    "# # Write the processed stream to the console (for testing purposes)\n",
    "# query = df_ready_for_dispatch.writeStream \\\n",
    "#     .format(\"console\") \\\n",
    "#     .outputMode(\"append\") \\\n",
    "#     .option(\"truncate\", \"false\") \\\n",
    "#     .option(\"checkpointLocation\", \"s3a://orders-for-dispatch/checkpoints/silver_stream\") \\\n",
    "#     .trigger(processingTime=\"10 seconds\") \\\n",
    "#     .start()\n",
    "\n",
    "# query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "route-optimizer-AqO2e-Ud-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
