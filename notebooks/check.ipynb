{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import os\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize AWS credentials from the .env file\n",
    "AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "AWS_SESSION_TOKEN = os.getenv('AWS_SESSION_TOKEN')\n",
    "AWS_REGION = os.getenv('AWS_REGION')\n",
    "ROUTIFIC_TOKEN = os.getenv('ROUTIFIC_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize boto3 client for Kinesis with your credentials\n",
    "kinesis_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    aws_session_token=AWS_SESSION_TOKEN,\n",
    "    region_name=AWS_REGION\n",
    ")\n",
    "\n",
    "# Initialize boto3 client for S3 with your credentials\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    aws_session_token=AWS_SESSION_TOKEN,\n",
    "    region_name=AWS_REGION \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/borja/Library/Caches/pypoetry/virtualenvs/route-optimizer-AqO2e-Ud-py3.11/lib/python3.11/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/borja/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/borja/.ivy2/jars\n",
      "io.delta#delta-core_2.12 added as a dependency\n",
      "org.apache.hadoop#hadoop-aws added as a dependency\n",
      "com.amazonaws#aws-java-sdk-bundle added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-abf9f713-f6dd-4d5f-88a3-35006721978d;1.0\n",
      "\tconfs: [default]\n",
      "\tfound io.delta#delta-core_2.12;2.4.0 in central\n",
      "\tfound io.delta#delta-storage;2.4.0 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.9.3 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.3.2 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central\n",
      ":: resolution report :: resolve 142ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]\n",
      "\tio.delta#delta-core_2.12;2.4.0 from central in [default]\n",
      "\tio.delta#delta-storage;2.4.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.9.3 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.3.2 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   6   |   0   |   0   |   0   ||   6   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-abf9f713-f6dd-4d5f-88a3-35006721978d\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 6 already retrieved (0kB/5ms)\n",
      "24/10/06 22:35:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Path to your local JAR files\n",
    "local_jars = \"/Users/borja/Documents/Somniumrema/projects/de/route_optimizer/jars/aws-java-sdk-kinesis-1.12.364.jar\"\n",
    "\n",
    "# Initialize Spark session with Delta and S3 settings\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KinesisToDeltaLake\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.3.2,com.amazonaws:aws-java-sdk-bundle:1.11.1026\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.session.token\", AWS_SESSION_TOKEN) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"134217728\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Optional: Adjust logging level\n",
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/06 22:35:40 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta table exists at path: s3a://orders-for-dispatch/silver\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "def check_delta_table_exists(spark, path):\n",
    "    try:\n",
    "        delta_table = DeltaTable.forPath(spark, path)\n",
    "        print(f\"Delta table exists at path: {path}\")\n",
    "        return True\n",
    "    except AnalysisException as e:\n",
    "        if \"is not a Delta table\" in str(e):\n",
    "            print(f\"No Delta table found at path: {path}\")\n",
    "            return False\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# Path to the Delta table in S3\n",
    "delta_table_path = \"s3a://orders-for-dispatch/silver\"\n",
    "\n",
    "# Check if the Delta table exists\n",
    "exists = check_delta_table_exists(spark, delta_table_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/06 22:35:46 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-------------------+\n",
      "|            order_id|         customer_id|      total_weight|      total_volume|       total_price|    order_timestamp|            status|               lat|                lon|\n",
      "+--------------------+--------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-------------------+\n",
      "|53db6ed4-90b8-4e9...|cus-96f56ac2-0c0f...| 50.73262649393302|190.44385165145806| 980.7997692409006|2024-10-06 21:41:21|READY_FOR_DISPATCH|40.549209890297696|-3.7227035885231077|\n",
      "|9de5440f-e96b-4ea...|cus-e9477353-df77...| 76.42521001573878|392.15686868126295|   948.53615868996|2024-10-06 21:40:57|READY_FOR_DISPATCH| 40.59354358838209|-3.7961876466288467|\n",
      "|75dc2792-2f2b-4b9...|cus-570786aa-85dc...|23.404358420138454|256.69045554496216| 581.7456700201304|2024-10-06 21:39:52|READY_FOR_DISPATCH| 40.61927751103353|-3.6799706163331223|\n",
      "|e0469e91-7703-4c8...|cus-5be9289d-6d63...|14.597707724499479| 389.8694549676822|253.22955492906556|2024-10-06 21:40:19|READY_FOR_DISPATCH|40.622941840713615| -3.603057881576992|\n",
      "|10a06a80-f36a-44b...|cus-c1aef5e9-8352...| 41.16115405994654|480.29077496769577| 662.5350100100675|2024-10-06 21:40:34|READY_FOR_DISPATCH|40.570689119574936| -3.835356187514956|\n",
      "|cae34ca8-0b3f-40c...|cus-b8637b4d-2898...| 58.46354277764553|307.56073074071094|140.78547379586917|2024-10-06 21:41:11|READY_FOR_DISPATCH|  40.5944520166111|-3.7275488553830263|\n",
      "+--------------------+--------------------+------------------+------------------+------------------+-------------------+------------------+------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "\n",
    "\n",
    "# Path to the Delta table in S3\n",
    "delta_table_path = \"s3a://orders-for-dispatch/silver\"\n",
    "\n",
    "# Read the Delta table\n",
    "delta_table = DeltaTable.forPath(spark, delta_table_path)\n",
    "\n",
    "# Convert Delta table to DataFrame\n",
    "df = delta_table.toDF()\n",
    "\n",
    "# Show the contents of the Delta table\n",
    "df.show()\n",
    "\n",
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchBucket",
     "evalue": "An error occurred (NoSuchBucket) when calling the GetObject operation: The specified bucket does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchBucket\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m object_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgold-optimization/dispatch_2024-10-06T19-59-03.117Z.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Download the JSON file from S3\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43ms3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobject_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m json_content \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Parse the JSON content\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/route-optimizer-AqO2e-Ud-py3.11/lib/python3.11/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/route-optimizer-AqO2e-Ud-py3.11/lib/python3.11/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mNoSuchBucket\u001b[0m: An error occurred (NoSuchBucket) when calling the GetObject operation: The specified bucket does not exist"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Initialize a session using Amazon S3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define the bucket name and the object key (file path)\n",
    "bucket_name = 'ready-for-dispatch'\n",
    "object_key = 'gold-optimization/dispatch_2024-10-06T19-59-03.117Z.json'\n",
    "\n",
    "# Download the JSON file from S3\n",
    "response = s3.get_object(Bucket=bucket_name, Key=object_key)\n",
    "json_content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "# Parse the JSON content\n",
    "data = json.loads(json_content)\n",
    "\n",
    "# Print the parsed data\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "route-optimizer-AqO2e-Ud-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
