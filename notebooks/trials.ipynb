{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_timestamp\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType\n",
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize AWS credentials from the .env file\n",
    "AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "AWS_SESSION_TOKEN = os.getenv('AWS_SESSION_TOKEN')\n",
    "AWS_REGION = os.getenv('AWS_REGION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 0\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- total_weight: double (nullable = true)\n",
      " |-- total_volume: double (nullable = true)\n",
      " |-- total_price: double (nullable = true)\n",
      " |-- order_timestamp: timestamp (nullable = true)\n",
      " |-- status: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      " |-- depot: string (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- batch_id: integer (nullable = true)\n",
      " |-- dispatched_timestamp: timestamp (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------+------------+-----------+---------------+------+---+---+-----+----+--------+--------------------+\n",
      "|order_id|customer_id|total_weight|total_volume|total_price|order_timestamp|status|lat|lon|depot|hour|batch_id|dispatched_timestamp|\n",
      "+--------+-----------+------------+------------+-----------+---------------+------+---+---+-----+----+--------+--------------------+\n",
      "+--------+-----------+------------+------------+-----------+---------------+------+---+---+-----+----+--------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Path to the Silver Delta table\n",
    "silver_table_path = \"s3a://dispatched-orders/gold/\"\n",
    "\n",
    "# Path to your local JAR files (optional, only necessary if running locally with custom JARs)\n",
    "local_jars = \"/Users/borja/Documents/Somniumrema/projects/de/route_optimizer/jars/aws-java-sdk-kinesis-1.12.364.jar\"\n",
    "\n",
    "# Initialize Spark session with Delta and S3 settings\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"DeltaLakeAggregation\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0,org.apache.hadoop:hadoop-aws:3.3.2,com.amazonaws:aws-java-sdk-bundle:1.11.1026\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", AWS_ACCESS_KEY_ID)\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", AWS_SECRET_ACCESS_KEY)\n",
    "    .config(\"spark.hadoop.fs.s3a.session.token\", AWS_SESSION_TOKEN)\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"134217728\")\n",
    "    .config(\"spark.driver.memory\", \"6g\")\n",
    "    .config(\"spark.executor.memory\", \"6g\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"100\")\n",
    "    .config(\"spark.databricks.delta.schema.autoMerge.enabled\", \"true\") \n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .getOrCreate())\n",
    "\n",
    "# Optional: Adjust logging level\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "\n",
    "# Read the Delta table from the Silver layer\n",
    "df_silver = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(silver_table_path)\n",
    "\n",
    "num_rows = df_silver.count()\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "# Show the schema and first few records to verify\n",
    "df_silver.printSchema()\n",
    "df_silver.show(100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading JSON file from S3: An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist.\n"
     ]
    }
   ],
   "source": [
    "# Specify the S3 path for the JSON file\n",
    "s3_json_path = \"s3a://dispatched-orders/to-optimizer/solution_batch_1_2024-10-16-10-56-52.json\"\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# Initialize the boto3 S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Specify the bucket and the file key (path to the file in the bucket)\n",
    "bucket_name = 'dispatched-orders'\n",
    "file_key = 'to-optimizer/solution_batch_1_2024-10-16T08-38-16.325Z.json'\n",
    "\n",
    "# Read the JSON file from S3\n",
    "try:\n",
    "    s3_response = s3_client.get_object(Bucket=bucket_name, Key=file_key)\n",
    "    json_content = s3_response['Body'].read().decode('utf-8')\n",
    "    \n",
    "    # Parse the JSON content\n",
    "    json_data = json.loads(json_content)\n",
    "    \n",
    "    # Print the parsed JSON data\n",
    "    print(json_data)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error reading JSON file from S3: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "route-optimizer-AqO2e-Ud-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
